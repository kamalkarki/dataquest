{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "income = pd.read_csv('/home/kamlesh/AnacondaProjects/Dataquest/Data/income.csv',index_col = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
       "0          2174             0              40   United-States       <=50K  \n",
       "1             0             0              13   United-States       <=50K  \n",
       "2             0             0              40   United-States       <=50K  \n",
       "3             0             0              40   United-States       <=50K  \n",
       "4             0             0              40            Cuba       <=50K  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
       "0   39          7   77516          9             13               4   \n",
       "1   50          6   83311          9             13               2   \n",
       "2   38          4  215646         11              9               0   \n",
       "3   53          4  234721          1              7               2   \n",
       "4   28          4  338409          9             13               2   \n",
       "\n",
       "   occupation  relationship  race  sex  capital_gain  capital_loss  \\\n",
       "0           1             1     4    1          2174             0   \n",
       "1           4             0     4    1             0             0   \n",
       "2           6             1     4    1             0             0   \n",
       "3           6             0     2    1             0             0   \n",
       "4          10             5     2    0             0             0   \n",
       "\n",
       "   hours_per_week  native_country  high_income  \n",
       "0              40              39            0  \n",
       "1              13              39            0  \n",
       "2              40              39            0  \n",
       "3              40              39            0  \n",
       "4              40               5            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a single column from text categories to numbers\n",
    "col = pd.Categorical(income[\"workclass\"])\n",
    "income[\"workclass\"] = col.codes\n",
    "#print(income[\"workclass\"].head(5))\n",
    "#print(income[\"workclass\"].head(5))\n",
    "\n",
    "for name in [\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\",\n",
    "             \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pd.Categorical(income[name])\n",
    "    #df.cc.astype('category').cat.codes\n",
    "    income[name] = col.codes\n",
    "    \n",
    "income.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last mission, we learned about the basics of decision trees, including entropy and information gain. In this mission, we'll build on those concepts to construct a full decision tree in Python and use it make predictions.\n",
    "\n",
    "We'll use the ID3 Algorithm for constructing decision trees to accomplish this. This algorithm involves recursion and an understanding of time complexity. If you're unfamiliar with these topics, we suggest trying our Data Structures and Algorithms course. We also suggest learning about lambda functions through our command line course.\n",
    "\n",
    "In general, recursion is the process of splitting a large problem into smaller chunks. Recursive functions will call themselves, then combine the results into a final output.\n",
    "\n",
    "Building a tree is a perfect use case for recursive algorithms. At each node, we'll call a recursive function that will split the data into two branches. Each branch will lead to a node, and the function will call itself to build the tree out.\n",
    "\n",
    "We've created a pseudocode version of the full ID3 Algorithm below. Pseudocode is a plain-text outline of a piece of code that explains how it works. Exploring the pseudocode for an algorithm is a good way to understand it better before trying to code it.\n",
    "\n",
    "    def id3(data, target, columns)\n",
    "    1 Create a node for the tree\n",
    "    2 If all values of the target attribute are 1, Return the node, with label = 1\n",
    "    3 If all values of the target attribute are 0, Return the node, with label = 0\n",
    "    4 Using information gain, find A, the column that splits the data best\n",
    "    5 Find the median value in column A\n",
    "    6 Split column A into values below or equal to the median (0), and values above the median (1)\n",
    "    7 For each possible value (0 or 1), vi, of A,\n",
    "    8    Add a new tree branch below Root that corresponds to rows of data where A = vi\n",
    "    9    Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "       10    Below this new branch add the subtree id3(data[A==vi], target, columns)\n",
    "       11 Return Root\n",
    "       \n",
    "       \n",
    "We've made a minor modification to the algorithm so that it only creates two branches from each node. This will simplify the process of constructing the tree, and make it easier to demonstrate the principles it involves.\n",
    "\n",
    "The recursive nature of the algorithm comes into play on line 10. Every node in the tree will call the id3() function, and the final tree will be the result of all of these calls       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last mission, we wrote functions to calculate entropy and information gain. We've loaded these functions in as calc_entropy() and calc_information_gain().\n",
    "\n",
    "Now we need a function that returns the name of the column we should use to split a data set. The function should take the name of the data set, the target column, and a list of columns we might want to split on as input.\n",
    "\n",
    "\n",
    "instructions\n",
    "\n",
    "    Write a function named find_best_column() that returns the name of a column to split the data on. We've started to define this function for you.\n",
    "\n",
    "    Use find_best_column() to find the best column on which to split income.\n",
    "        The target is the high_income column, and the potential columns to split with are in the list columns below.\n",
    "        Assign the result to income_split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital_status\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import math\n",
    "def calc_entropy(column):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a pandas series, list, or numpy array.\n",
    "    \"\"\"\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = numpy.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy\n",
    "def calc_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    # Find the median of the column we're splitting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    # Make two subsets of the data, based on the median\n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "\n",
    "def find_best_column(data, target_name, columns):\n",
    "    # Fill in the logic here to automatically find the column in columns to split on\n",
    "    # data is a dataframe\n",
    "    # target_name is the name of the target variable\n",
    "    # columns is a list of potential columns to split on\n",
    "    information_gain =[]\n",
    "    for i in columns:\n",
    "        gain = calc_information_gain( data,i, target_name )\n",
    "        information_gain.append(gain)\n",
    "    split = information_gain.index(max(information_gain))  \n",
    "    return columns[split]\n",
    "\n",
    "# A list of columns to potentially split income with\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\",\n",
    "           \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "income_split = find_best_column( income,'high_income',columns)\n",
    "#income_split = columns[income_split]\n",
    "print(income_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build up to making the full id3() function by creating a simpler algorithm that we can extend. Here's what that algorithm looks like in pseudocode:\n",
    "\n",
    "def id3(data, target, columns)\n",
    "\n",
    "    1 Create a node for the tree\n",
    "\n",
    "    2 If all values of the target attribute are 1, add 1 to counter_1\n",
    "\n",
    "    3 If all values of the target attribute are 0, add 1 to counter_0\n",
    "\n",
    "    4 Using information gain, find A, the column that splits the data best\n",
    "\n",
    "    5 Find the median value in column A\n",
    "\n",
    "    6 Split A into values below or equal to the median (0), and values above the median (1)\n",
    "\n",
    "    7 For each possible value (0 or 1), vi, of A,\n",
    "\n",
    "    8    Add a new tree branch below Root that corresponds to rows of data where A = vi\n",
    "\n",
    "    9    Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "\n",
    "   10    Below this new branch, add the subtree id3(data[A==vi], target, columns)\n",
    "\n",
    "   11 Return Root\n",
    "\n",
    "This version is very similar to the algorithm above, but lines 2 and 3 are different. Rather than storing the entire tree (which is a bit complicated), we'll just tally how many leaves end up with the label 1, and how many end up with the label 0.\n",
    "\n",
    "We'll replicate this algorithm in code, and apply it to the same data set we just stepped through on a previous screen:\n",
    "\n",
    "\n",
    "    high_income    age    marital_status\n",
    "\n",
    "    0              20     0\n",
    "\n",
    "    0              60     2\n",
    "\n",
    "    0              40     1\n",
    "    \n",
    "    1              25     1\n",
    "\n",
    "    1              35     2\n",
    "    \n",
    "    1              55     1\n",
    "    \n",
    "   Instructions\n",
    "       Read the id3() function below and fill in the lines that say \"Insert code here...\".\n",
    "\n",
    "        The function should append 1 to label_1s if the node should be a leaf, and only has 1s for high_income.\n",
    "        It should append 0 to label_0s if the node should be a leaf, and only has 0s for high_income.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   high_income  age  marital_status\n",
      "0            0   20               0\n",
      "   high_income  age  marital_status\n",
      "3            1   25               1\n",
      "   high_income  age  marital_status\n",
      "4            1   35               2\n",
      "   high_income  age  marital_status\n",
      "2            0   40               1\n",
      "   high_income  age  marital_status\n",
      "5            1   55               1\n",
      "   high_income  age  marital_status\n",
      "1            0   60               2\n",
      "[1, 1, 1]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# We'll use lists to store our labels for nodes (when we find them)\n",
    "# Lists can be accessed inside our recursive function, whereas integers can't.  \n",
    "# Look at the python missions on scoping for more information on this topic\n",
    "label_1s = []\n",
    "label_0s = []\n",
    "\n",
    "def id3(data, target, columns):\n",
    "    # The pandas.unique method will return a list of all the unique values in a series\n",
    "    unique_targets = pd.unique(data[target])\n",
    "    \n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here to append 1 to label_1s or 0 to label_0s, based on what we should label the node\n",
    "        # See lines 2 and 3 in the algorithm\n",
    "        if 1 in unique_targets:\n",
    "            print( data )\n",
    "            label_1s.append( 1 )\n",
    "        else:\n",
    "            print(data)\n",
    "            label_0s.append ( 0)\n",
    "            \n",
    "        \n",
    "        # Returning here is critical -- if we don't, the recursive tree will never finish, and run forever\n",
    "        # See our example above for when we returned\n",
    "        return \n",
    "    \n",
    "    # Find the best column to split on in our data\n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    # Find the median of the column\n",
    "    #print('best column is')\n",
    "    #print( best_column)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Create the two splits\n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    \n",
    "    # Loop through the splits and call id3 recursively\n",
    "    for split in [left_split, right_split]:\n",
    "        # Call id3 recursively to process each branch\n",
    "        id3(split, target, columns)\n",
    "    \n",
    "# Create the data set that we used in the example on the last screen\n",
    "data = pd.DataFrame([\n",
    "    [0,20,0],\n",
    "    [0,60,2],\n",
    "    [0,40,1],\n",
    "    [1,25,1],\n",
    "    [1,35,2],\n",
    "    [1,55,1]\n",
    "    ])\n",
    "# Assign column names to the data\n",
    "data.columns = [\"high_income\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"])\n",
    "\n",
    "print(label_1s)\n",
    "print(label_0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can store the entire tree, rather than the leaf labels only. We'll use nested dictionaries to do this. We can represent the root node with a dictionary, and branches with the keys left and right. We'll store the column we're splitting on as the key column, and the median value as the key median. Finally, we can store the label for a leaf as the key label. We'll also number each node as we go along using the number key.\n",
    "\n",
    "We'll use the same data set we've been working with:\n",
    "\n",
    "\n",
    "    `high_income   age    marital_status\n",
    "\n",
    "    0              20     0\n",
    "\n",
    "    0              60     2\n",
    "\n",
    "    0              40     1\n",
    "\n",
    "    1              25     1\n",
    "\n",
    "    1              35     2\n",
    "\n",
    "    1              55     1\n",
    "\n",
    "Here's what the dictionary for the decision tree will look like:\n",
    "\n",
    "{  \n",
    "   \"left\":{  \n",
    "      \"left\":{  \n",
    "         \"left\":{  \n",
    "            \"number\":4,\n",
    "            \"label\":0\n",
    "         },\n",
    "         \"column\":\"age\",\n",
    "         \"median\":22.5,\n",
    "         \"number\":3,\n",
    "         \"right\":{  \n",
    "            \"number\":5,\n",
    "            \"label\":1\n",
    "         }\n",
    "      },\n",
    "      \"column\":\"age\",\n",
    "      \"median\":25.0,\n",
    "      \"number\":2,\n",
    "      \"right\":{  \n",
    "         \"number\":6,\n",
    "         \"label\":1\n",
    "      }\n",
    "   },\n",
    "   \"column\":\"age\",\n",
    "   \"median\":37.5,\n",
    "   \"number\":1,\n",
    "   \"right\":{  \n",
    "      \"left\":{  \n",
    "         \"left\":{  \n",
    "            \"number\":9,\n",
    "            \"label\":0\n",
    "         },\n",
    "         \"column\":\"age\",\n",
    "         \"median\":47.5,\n",
    "         \"number\":8,\n",
    "         \"right\":{  \n",
    "            \"number\":10,\n",
    "            \"label\":1\n",
    "         }\n",
    "      },\n",
    "      \"column\":\"age\",\n",
    "      \"median\":55.0,\n",
    "      \"number\":7,\n",
    "      \"right\":{  \n",
    "         \"number\":11,\n",
    "         \"label\":0\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "If we look at node 2 (the left branch of the root node), we see that it matches the hand exercise we completed a few screens ago. It splits, creating a right branch (node 6) with the label 1, and a left branch (node 3) that splits again.\n",
    "\n",
    "In order to keep track of the tree, we'll need to make some modifications to id3(). The first modification involves changing the definition to pass in the tree dictionary:\n",
    "\n",
    "def id3(data, target, columns, tree)\n",
    "\n",
    "    1 Create a node for the tree\n",
    "\n",
    "    2 Number the node\n",
    "\n",
    "    3 If all of the values of the target attribute are 1, assign 1 to the label key in tree\n",
    "\n",
    "    4 If all of the values of the target attribute are 0, assign 0 to the label key in tree\n",
    "\n",
    "    5 Using information gain, find A, the column that splits the data best\n",
    "\n",
    "    6 Find the median value in column A\n",
    "\n",
    "    7 Assign the column and median keys in tree\n",
    "\n",
    "    8 Split A into values less than or equal to the median (0), and values above the median (1)\n",
    "\n",
    "    9 For each possible value (0 or 1), vi, of A,\n",
    "\n",
    "       10    Add a new tree branch below Root that corresponds to rows of data where A = vi\n",
    "\n",
    "       11    Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "\n",
    "       12    Create a new key with the name corresponding to the side of the split (0=left, 1=right).  The value of this key should be an empty dictionary.\n",
    "\n",
    "       13    Below this new branch, add the subtree id3(data[A==vi], target, columns, tree[split_side])\n",
    "\n",
    "       14 Return Root\n",
    "\n",
    "Under this approach, we're now passing the tree dictionary into our id3 function and setting some keys on it. One complexity is in how we're creating the nested dictionary. For the left split, we're adding a key to the tree dictionary that looks like this:\n",
    "\n",
    "tree[\"left\"] = {}\n",
    "\n",
    "For the right side, we're adding:\n",
    "\n",
    "tree[\"right\"] = {}\n",
    "\n",
    "Now that we've added this key, we're able to pass our new dictionary into the recursive call to id3(). While this new dictionary will be the dictionary for that specific node, it will be tied back to the parent dictionary (because it's a key of the original dictionary).\n",
    "\n",
    "This process will continue building up the nested dictionary. We'll be able to access the entire dictionary using the variable tree we define before the function. Think of each recursive call as building a piece of the tree, which we can then access after all of the functions have terminated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'column': 'age',\n",
       " 'left': {'column': 'age',\n",
       "  'left': {'column': 'age',\n",
       "   'left': {'label': 0, 'number': 4},\n",
       "   'median': 22.5,\n",
       "   'number': 3,\n",
       "   'right': {'label': 1, 'number': 5}},\n",
       "  'median': 25.0,\n",
       "  'number': 2,\n",
       "  'right': {'label': 1, 'number': 6}},\n",
       " 'median': 37.5,\n",
       " 'number': 1,\n",
       " 'right': {'column': 'age',\n",
       "  'left': {'column': 'age',\n",
       "   'left': {'label': 0, 'number': 9},\n",
       "   'median': 47.5,\n",
       "   'number': 8,\n",
       "   'right': {'label': 1, 'number': 10}},\n",
       "  'median': 55.0,\n",
       "  'number': 7,\n",
       "  'right': {'label': 0, 'number': 11}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to hold the tree  \n",
    "# It has to be outside of the function so we can access it later\n",
    "tree = {}\n",
    "\n",
    "# This list will let us number the nodes  \n",
    "# It has to be a list so we can access it inside the function\n",
    "nodes = []\n",
    "\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pd.unique(data[target])\n",
    "    \n",
    "    # Assign the number key to the node dictionary\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        # Insert code here that assigns the \"label\" field to the node dictionary\n",
    "        if 1 in unique_targets:\n",
    "            tree[\"label\"] = 1\n",
    "        else :\n",
    "            tree[\"label\"] = 0\n",
    "        \n",
    "        return\n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    # Insert code here that assigns the \"column\" and \"median\" fields to the node dictionary\n",
    "    tree[\"column\"] = best_column\n",
    "    tree[\"median\"] = column_median\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    for name, split in split_dict:\n",
    "        #below lines make dictionary inside dictionary \n",
    "        # tree[\"left\"] = {}\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "# Call the function on our data to set the counters properly\n",
    "id3(data, \"high_income\", [\"age\", \"marital_status\"], tree)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree dictionary shows all of the relevant information, but it doesn't look very nice. We can fix its appearance by printing it out in a nicer format.\n",
    "\n",
    "To do this, we'll need to recursively iterate through our tree dictionary. Any dictionary that has a label key is a leaf. Whenever we find one, we'll print out the label. Otherwise, we'll loop through the tree's left and right keys and recursively call the same function.\n",
    "\n",
    "We also need to keep track of a depth variable. This variable will allow us to use indentation to indicate the order of the nodes. Before we print anything out, we'll prefix it with the number of spaces corresponding to the depth variable.\n",
    "\n",
    "Here's the pseudocode:\n",
    "\n",
    "    def print_node(tree, depth):\n",
    "    1 Check for the presence of the \"label\" key in the tree\n",
    "    2     If found, print the label and return\n",
    "    3 Print out the tree's \"column\" and \"median\" keys\n",
    "    4 Iterate through the tree's \"left\" and \"right\" keys\n",
    "    5     Recursively call print_node(tree[key], depth+1)\n",
    "    \n",
    "Instructions:    \n",
    "Fill in the gaps in the print_node() function that say \"Insert code here...\".\n",
    "\n",
    "    Your code should iterate through both branches of the branches list (in order), and recursively call print_node().\n",
    "        Don't forget to increment depth when you call print_node.\n",
    "\n",
    "Call print_node(), and pass in tree and depth 0.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age > 37.5\n",
      "    age > 25.0\n",
      "        age > 22.5\n",
      "            Leaf: Label 0\n",
      "            Leaf: Label 1\n",
      "        Leaf: Label 1\n",
      "    age > 55.0\n",
      "        age > 47.5\n",
      "            Leaf: Label 0\n",
      "            Leaf: Label 1\n",
      "        Leaf: Label 0\n"
     ]
    }
   ],
   "source": [
    "def print_with_depth(string, depth):\n",
    "    # Add space before a string\n",
    "    prefix = \"    \" * depth\n",
    "    # Print a string, and indent it appropriately\n",
    "    print(\"{0}{1}\".format(prefix, string))\n",
    "\n",
    "def print_node(tree, depth):\n",
    "    # Check for the presence of \"label\" in the tree\n",
    "    if \"label\" in tree:\n",
    "        # If found, then this is a leaf, so print it and return\n",
    "        print_with_depth(\"Leaf: Label {0}\".format(tree[\"label\"]), depth)\n",
    "        # This is critical -- without it, you'll get infinite recursion\n",
    "        return\n",
    "    # Print information about what the node is splitting on\n",
    "    print_with_depth(\"{0} > {1}\".format(tree[\"column\"], tree[\"median\"]), depth)\n",
    "    \n",
    "    # Create a list of tree branches\n",
    "    #branches = [tree[\"left\"], tree[\"right\"]]\n",
    "    for branch in [tree[\"left\"], tree[\"right\"]]:\n",
    "        print_node(branch, depth+1)    \n",
    "    # Insert code here to recursively call print_node on each branch\n",
    "    # Don't forget to increment depth when you pass it in\n",
    "\n",
    "print_node(tree, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the scikit-learn package to fit a decision tree. The interface is very similar to other algorithms we've fit in the past.\n",
    "\n",
    "We use the DecisionTreeClassifier class for classification problems, and DecisionTreeRegressor for regression problems. The sklearn.tree package includes both of these classes.\n",
    "\n",
    "In this case, we're predicting a binary outcome, so we'll use a classifier.\n",
    "\n",
    "The first step is to train the classifier on the data. We'll use the fit method on a classifier to do this.\n",
    "\n",
    "Instructions\n",
    "\n",
    "\n",
    "    Pass in income[columns] so that we only use the named columns as predictors.\n",
    "    The target is the high_income column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#import data\n",
    "income_df = pd.read_csv('/home/kamlesh/AnacondaProjects/Dataquest/Data/income.csv',index_col=False)\n",
    "all_columns = [ \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \n",
    "           \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to make sure the results are consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "#convert categorical values\n",
    "for f in all_columns:\n",
    "    code = pd.Categorical(income_df[f])\n",
    "    income_df[f] = code.codes\n",
    "    \n",
    "clf.fit(income_df[all_columns], income_df['high_income'] )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the rows in income with a position up to train_max_row (but not including it) will be part of the training set.\n",
    "\n",
    "    Make a new dataframe called train containing all of these rows.\n",
    "    Make a dataframe called test containing all of the rows with a position greater than or equal to train_max_row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# Shuffle the rows  \n",
    "# This permutes the index randomly using numpy.random.permutation\n",
    "# Then, it reindexes the dataframe with the result\n",
    "# The net effect is to put the rows into random order\n",
    "income_df = income.reindex(numpy.random.permutation(income_df.index))\n",
    "\n",
    "train_max_row = math.floor(income_df.shape[0] * .8)\n",
    "income_df.shape\n",
    "train = income_df[:train_max_row ] \n",
    "test = income_df[train_max_row :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69346563247461923"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "roc_auc_score( test['high_income'], predictions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947124450144\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train[columns])\n",
    "error = roc_auc_score( train['high_income'],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main ways to combat overfitting:\n",
    "\n",
    "    \"Prune\" the tree after we build it to remove unnecessary leaves.\n",
    "    Use ensembling to blend the predictions of many trees.\n",
    "    Restrict the depth of the tree while we're building it.\n",
    "\n",
    "While we'll explore all of these, we'll look at the third method first.\n",
    "\n",
    "Limiting tree depth during the building process will result in more general rules. This prevents the tree from overfitting.\n",
    "\n",
    "We can restrict tree depth by adding a few parameters when we initialize the DecisionTreeClassifier class:\n",
    "\n",
    "    max_depth - Globally restricts how deep the tree can go\n",
    "    min_samples_split - The minimum number of rows a node should have before it can be split; if this is set to 2, for example, then nodes with 2 rows won't be split, and will become leaves instead\n",
    "    min_samples_leaf - The minimum number of rows a leaf must have\n",
    "    min_weight_fraction_leaf - The fraction of input rows a leaf must have\n",
    "    max_leaf_nodes - The maximum number of total leaves; this will cap the count of leaf nodes as the tree is being built\n",
    "\n",
    "Some of these parameters aren't compatible, however. For example, we can't use max_depth and max_leaf_nodes together.\n",
    "\n",
    "Now that we know what to tweak, let's improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699561714515\n"
     ]
    }
   ],
   "source": [
    "# Decision trees model from the last screen\n",
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split= 13)\n",
    "clf.fit( train[columns], train['high_income'] )\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score( test['high_income'], predictions )\n",
    "print(test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "\n",
    "    Set min_samples_split to 13 when creating the DecisionTreeClassifier.\n",
    "    Make predictions on the training set, compute the AUC, and assign it to train_auc.\n",
    "    Make predictions on the test set, compute the AUC, and assign it to test_auc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841898573725\n",
      "0.711079143953\n"
     ]
    }
   ],
   "source": [
    "# Decision trees model from the last screen\n",
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split = 13)\n",
    "clf.fit( train[columns], train['high_income'])\n",
    "\n",
    "train_predictions = clf.predict(train[columns] )\n",
    "train_auc = roc_auc_score( train['high_income'],train_predictions ) \n",
    "                                \n",
    "test_predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score( test['high_income'], test_predictions )\n",
    "\n",
    "print(train_auc)\n",
    "print(test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "    Set max_depth to 7 and min_samples_split to 13 when creating the DecisionTreeClassifier.\n",
    "    Make predictions on the training set, compute the AUC, and assign it to train_auc.\n",
    "    Make predictions on the test set, compute the AUC, and assign it to test_auc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743634499673\n",
      "0.748037708309\n"
     ]
    }
   ],
   "source": [
    "# The first decision trees model we trained and tested\n",
    "clf = DecisionTreeClassifier(random_state=1,max_depth = 7 ,min_samples_split = 13)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both roc for training and test is same, lets play aggresively\n",
    "\n",
    "    Set max_depth to 2 and min_samples_split to 100 when creating the DecisionTreeClassifier.\n",
    "    Make predictions on the training set, compute the AUC, and assign it to train_auc.\n",
    "    Make predictions on the test set, compute the AUC, and assign it to test_auc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655313848188\n",
      "0.662450804216\n"
     ]
    }
   ],
   "source": [
    "# The first decision tree model we trained and tested\n",
    "clf = DecisionTreeClassifier(random_state=1,max_depth= 2,min_samples_split = 100)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(test[\"high_income\"], predictions)\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train[\"high_income\"], train_predictions)\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By artificially restricting the depth of our tree, we prevent it from creating a model that's complex enough to correctly categorize some of the rows. If we don't perform the artificial restrictions, however, the tree becomes too complex, fits quirks in the data that only exist in the training set, and doesn't generalize to new data.\n",
    "\n",
    "This is known as the bias-variance tradeoff. Imagine that we take a random sample of training data and create many models. If the models' predictions for the same row are far apart from each other, we have high variance. Imagine this time that we take a random sample of the training data and create many models. If the models' predictions for the same row are close together but far from the actual value, then we have high bias.\n",
    "\n",
    "High bias can cause underfitting -- if a model is consistently failing to predict the correct value, it may be that it's too simple to model the data faithfully.\n",
    "\n",
    "High variance can cause overfitting. If a model varies its predictions significantly based on small changes in the input data, then it's likely fitting itself to quirks in the training data, rather than making a generalizable model.\n",
    "\n",
    "We call this the bias-variance tradeoff because decreasing one characteristic will usually increase the other. This is a limitation of all machine learning algorithms. If you'd like to read more about the tradeoff, check out programmer Scott Fortmann-Roe's post on this topic.\n",
    "\n",
    "Decision trees typically suffer from high variance. The entire structure of a decision tree can change if we make a minor alteration to its training data. By restricting the depth of the tree, we increase the bias and decrease the variance. If we restrict the depth too much, we increase bias to the point where it will underfit.\n",
    "\n",
    "You'll generally need to use your intuition and manually tweak parameters to get the \"right\" fit.\n",
    "\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can induce variance and see what happens with a decision tree. To introduce noise into the data, we'll add a column of random values. A model with high variance (like a decision tree) will pick up on this noise, and overfit to it. This is because models with high variance are very sensitive to small changes in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975076161435\n",
      "0.691406001394\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(1)\n",
    "\n",
    "# Generate a column containing random numbers from 0 to 4\n",
    "income_df[\"noise\"] = numpy.random.randint(4, size=income.shape[0])\n",
    "\n",
    "# Adjust \"columns\" to include the noise column\n",
    "columns = [\"noise\", \"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \n",
    "           \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "# Make new train and test sets\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "train = income_df.iloc[:train_max_row]\n",
    "test = income_df.iloc[train_max_row:]\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "#training is done, now predict\n",
    "predictions_train = clf.predict( train[columns])\n",
    "train_auc = roc_auc_score( train['high_income'], predictions_train )\n",
    "\n",
    "# now test lo predict karo\n",
    "predictions_test = clf.predict( test[columns] )\n",
    "#now compare\n",
    "test_auc = roc_auc_score(test['high_income'], predictions_test  )\n",
    "\n",
    "print(train_auc)\n",
    "print(test_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the random noise column causes significant overfitting. Our test set accuracy decreases to .691, and our training set accuracy increases to .975.\n",
    "\n",
    "One way to prevent overfitting is to block the tree from growing beyond a certain depth (we tried this before). Another technique is called pruning. Pruning involves building a full tree, and then removing the leaves that don't add to prediction accuracy. Pruning prevents a model from becoming overly complex. It can result in a simpler model that has higher accuracy on the testing set.\n",
    "\n",
    "Data scientists use pruning less often than parameter optimization (what we just did) and ensembling. It's still an important technique, though, and we'll cover it in more depth down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's go over the main advantages and disadvantages of using decision trees. The main advantages of using decision trees is that they're:\n",
    "\n",
    "    Easy to interpret\n",
    "    Relatively fast to fit and make predictions\n",
    "    Able to handle multiple types of data\n",
    "    Able to pick up nonlinearities in data, and usually fairly accurate\n",
    "\n",
    "The main disadvantage of using decision trees is their tendency to overfit.\n",
    "\n",
    "Decision trees are a good choice for tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing.\n",
    "\n",
    "The most powerful way to reduce decision tree overfitting is to create ensembles of trees. The random forest algorithm is a popular choice for doing this. In cases where prediction accuracy is the most important consideration, random forests usually perform better.\n",
    "\n",
    "In the next mission, we'll explore the random forest algorithm in greater depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past three missions, we learned about decision trees, and looked at ways to reduce overfitting. The most powerful tool for reducing decision tree overfitting is called the random forest algorithm. In this mission, we'll learn how to construct and apply random forests.\n",
    "\n",
    "We'll continue using the same 1994 census data set on U.S. income. It contains information on marital status, age, type of work, and more. The target column, high_income, records salaries less than or equal to 50k a year (0), and more than 50k a year (1).\n",
    "\n",
    "You can download the data from the University of California, Irvine's website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a kind of ensemble model. Ensembles combine the predictions of multiple models to create a more accurate final prediction. We'll make a simple ensemble to see how they work.\n",
    "\n",
    "Let's create two decision trees with slightly different parameters:\n",
    "\n",
    "    One with min_samples_leaf set to 2\n",
    "    One with max_depth set to 5\n",
    "\n",
    "Then, we'll check their accuracies separately. On the next screen, we'll combine their predictions and compare the combined accuracy with the individual accuracies of both trees.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "    Fit both clf and clf2 to the data.\n",
    "        Use train[columns] as the predictors, and train[\"high_income\"] as the target.\n",
    "\n",
    "    Make predictions on the test set predictors (test[columns]) using both clf and clf2.\n",
    "\n",
    "    For both sets of predictions, compute the AUC between the predictions and the actual values (test[\"high_income\"]) using the roc_auc_score function.\n",
    "    Use the print() function to display the AUC values for both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693844639612\n",
      "0.67422521704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "#initialized\n",
    "clf_1 = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "clf_2 = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "\n",
    "#fitting the model\n",
    "clf_1.fit(train[columns], train[\"high_income\"])\n",
    "clf_2.fit(train[columns], train['high_income'])\n",
    "\n",
    "#predections on test data\n",
    "test_predictions_1 = clf_1.predict( test[columns] )\n",
    "test_predictions_2 = clf_2.predict( test[columns])\n",
    "\n",
    "print( roc_auc_score( test['high_income'], test_predictions_1 ) )\n",
    "\n",
    "print( roc_auc_score( test['high_income'], test_predictions_2 ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have multiple classifiers making predictions, we can treat each set of predictions as a column in a matrix. Here's an example where we have Decision Tree 1 (DT1), Decision Tree 2 (DT2), and Decision Tree 3 (DT3):\n",
    "\n",
    "    DT1     DT2    DT3\n",
    "\n",
    "    0       1      0\n",
    "\n",
    "    1       1      1\n",
    "\n",
    "    0       0      1\n",
    "    \n",
    "    1       0      0\n",
    "\n",
    "Whenever we add more models to our ensemble, we just add more columns to the combined predictions. Ultimately, we don't want this matrix, though -- we want one prediction per row in the training data. To accomplish this, we'll need to create rules to convert each row of our matrix of predictions into a single number.\n",
    "\n",
    "We want to create a Final Prediction vector that looks like this:\n",
    "\n",
    "    DT1     DT2    DT3    Final Prediction\n",
    "\n",
    "    0       1      0      0\n",
    "\n",
    "    1       1      1      1\n",
    "\n",
    "    0       0      1      0\n",
    "\n",
    "    1       0      0      0\n",
    "\n",
    "There are many ways to get from the output of multiple models to a final vector of predictions. One method is majority voting, in which each classifier gets a \"vote,\" and the most commonly voted value for each row \"wins.\" This only works if there are more than two classifiers (and ideally an odd number, so we don't have to write a rule to break ties). Majority voting is what we applied in the example above.\n",
    "\n",
    "Because we only had two classifiers on the last screen, we'll have to use a different method to combine predictions. We'll take the mean of all of the items in a row. Right now, we're using the predict() method, which returns either 0 or 1. predict() returns something like this:\n",
    "\n",
    "    0\n",
    "\n",
    "    1\n",
    "\n",
    "    0\n",
    "\n",
    "    1\n",
    "\n",
    "We can use the RandomForestClassifier.predict_proba() method instead, which will predict a probability from 0 to 1 that a given class is the right one for a row. Because 0 and 1 are our two classes, we'll get a matrix containing the number of rows in the income dataframe, and two columns. predict_proba() will return a result that looks like this:\n",
    "\n",
    "    0     1\n",
    "\n",
    "    .7    .3\n",
    "\n",
    "    .2    .8\n",
    "\n",
    "    .1    .9\n",
    "\n",
    "Each row will correspond to a prediction. The first column is the probability that the prediction is a 0, and the second column is the probability that the prediction is a 1. Each row adds up to 1.\n",
    "\n",
    "If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector, instead of just 0 or 1.\n",
    "\n",
    "Then we can add together all of the vectors we get through this method, and divide the sum by the total number of vectors to get the mean prediction made across the entire ensemble for a particular row. Finally, we round off to get a 0 or 1 prediction for the row.\n",
    "\n",
    "If we use the predict_proba() method on both classifiers from the last screen to generate probabilities, take the mean for each row, and then round the results, we'll get ensemble predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726882697355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "final = ( predictions + predictions2 )/2\n",
    "final = np.round( final )\n",
    "\n",
    "print( roc_auc_score( test['high_income'], final ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling with Bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the previous screen, the combined predictions of the two trees had a higher AUC than that of either tree on its own:\n",
    " \n",
    "    settings \t                  test AUC\n",
    "    min_samples_leaf: 2 \t      0.69\n",
    "    max_depth: 5 \t              0.67\n",
    "    combined predictions \t      0.72\n",
    "\n",
    "To intuitively understand why this makes sense, think about two people with the same level of talent. One learned programming in college, while the other learned it on her own (let's say using Dataquest!).\n",
    "\n",
    "If we give both of them the same project, they'll approach it in slightly different ways, due to their different experiences. They may both produce code that achieves the same result, but one may run faster in certain areas. The other may have a better interface. Even though both of them have about the same level of talent, their solutions are stronger in different areas because they approached the problem differently.\n",
    "\n",
    "If we combine the best parts of both of their projects, we'll end up with a stronger combined project.\n",
    "\n",
    "Ensembling is exactly the same. The models are approaching the same problem in slightly different ways, and building different trees because we used different parameters for each one. Each tree makes different predictions in different areas. Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.\n",
    "\n",
    "The more \"diverse\" or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy). Ensembling a decision tree and a logistic regression model, for example, will result in stronger predictions than ensembling two decision trees with similar parameters. That's because those two models use very different approaches to arrive at their answers.\n",
    "\n",
    "On the other hand, if the models we ensemble are very similar in how they make predictions, ensembling will result in a negligible boost.\n",
    "\n",
    "Ensembling models with very different accuracies generally won't improve overall accuracy. Ensembling a model with a .75 AUC and a model with a .85 AUC on a test set will usually result in an AUC somewhere in between the two original values. There's a way around this that we'll discuss later on, which we call weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857570910145\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample in every loop\n",
    "    # That would make all of our trees the same\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    x = clf.predict_proba( test[columns])[:,1] \n",
    "    \n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "    \n",
    "combined = numpy.sum( predictions, axis= 0)\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  5.]\n"
     ]
    }
   ],
   "source": [
    "test_list =[[3,5],[3,5]]\n",
    "combined = numpy.sum(test_list, axis=0) / 2\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bagging example from the previous screen, we gained some accuracy over a single decision tree. To be exact, we achieved an AUC score of around .733 with bagging, which is an improvement over the AUC score of .688 we got without bagging:\n",
    "\n",
    "    settings                test AUC\n",
    "    min_samples_leaf: 2 \t0.688\n",
    "    max_depth: 2 \t        0.676\n",
    "    combined predictions \t0.72\n",
    "    min_samples_leaf: 2, with bagging \t0.85\n",
    "\n",
    "Let's go back to the decision tree algorithm we explored two missions ago to explain random feature subsets:\n",
    "\n",
    "    First we pick the maximum number of features we want to evaluate each time we split the tree.\n",
    "        This has to be less than the total number of columns in the data.\n",
    "    Every time we split, we pick a random sample of features from the data.\n",
    "    Then we compute the information gain for each feature in our random sample, and pick the one with the highest information gain to split on.\n",
    "\n",
    "To understand how splits work, let's look at information gain or entropy. Entropy is the measure of \"disorder\" in the data set. If a dataset has all the same labels, they'll have low entropy. If all the labels are different, they'll have high entropy. Splits that give us more information about the data, will ideally minimize entropy. In other words, the tree will ideally split the labels into distinct groups with as little mixture possible. This'll allow the splits to give our tree more predictive power.\n",
    "\n",
    "We're repeating the same process to select the optimal split that minimizes entropy for a node. However, we'll only evaluate a constrained set of features that we select randomly. This introduces variation into the trees, and makes for more powerful ensembles.\n",
    "\n",
    "Below is the ID3 algorithm that we developed earlier. We'll modify it to only consider a certain subset of the features.\n",
    "instructions\n",
    "\n",
    "    Modify find_best_column to select a random sample from columns before computing information gain.\n",
    "        Look where it says Insert code here.\n",
    "        Each subset will have 2 items in it.\n",
    "        You can use numpy.random.choice() to select a random sample.\n",
    "        The first input is the list you're picking from, and the second is the number of items you want to pick.\n",
    "\n",
    "    Be careful not to overwrite columns when you do the selection.\n",
    "        The random sample should go in a different variable, and you'll have to modify some of the surrounding code to match.\n",
    "\n",
    "    Use the print() function to display tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': 1, 'column': 'employment', 'median': 4.5, 'left': {'number': 2, 'column': 'age', 'median': 25.0, 'left': {'number': 3, 'column': 'age', 'median': 22.5, 'left': {'number': 4, 'label': 0}, 'right': {'number': 5, 'label': 1}}, 'right': {'number': 6, 'label': 0}}, 'right': {'number': 7, 'column': 'age', 'median': 40.0, 'left': {'number': 8, 'column': 'age', 'median': 37.5, 'left': {'number': 9, 'label': 1}, 'right': {'number': 10, 'label': 0}}, 'right': {'number': 11, 'label': 1}}}\n"
     ]
    }
   ],
   "source": [
    "# Create the data set that we used two missions ago\n",
    "import pandas \n",
    "data = pandas.DataFrame([\n",
    "    [0,4,20,0],\n",
    "    [0,4,60,2],\n",
    "    [0,5,40,1],\n",
    "    [1,4,25,1],\n",
    "    [1,5,35,2],\n",
    "    [1,5,55,1]\n",
    "    ])\n",
    "data.columns = [\"high_income\", \"employment\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Set a random seed to make the results reproducible\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# The dictionary to store our tree\n",
    "tree = {}\n",
    "nodes = []\n",
    "\n",
    "# The function to find the column to split on\n",
    "def find_best_column(data, target_name, columns):\n",
    "    information_gains = []\n",
    "    \n",
    "    # Insert your code here\n",
    "    cols = numpy.random.choice(columns, 2)\n",
    "    for col in columns:\n",
    "        information_gain = calc_information_gain(data, col, \"high_income\")\n",
    "        information_gains.append(information_gain)\n",
    "\n",
    "    # Find the name of the column with the highest gain\n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    highest_gain = columns[highest_gain_index]\n",
    "    return highest_gain\n",
    "\n",
    "# The function to construct an ID3 decision tree\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            tree[\"label\"] = 0\n",
    "        elif 1 in unique_targets:\n",
    "            tree[\"label\"] = 1\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    tree[\"column\"] = best_column\n",
    "    tree[\"median\"] = column_median\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "\n",
    "# Run the ID3 algorithm on our data set and print the resulting tree\n",
    "id3(data, \"high_income\", [\"employment\", \"age\", \"marital_status\"], tree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also repeat our random subset selection process in scikit-learn. We just set the splitter parameter on DecisionTreeClassifier to \"random\", and the max_features parameter to \"auto\". If we have N columns, this will pick a subset of features of size N‾‾√\n",
    "\n",
    ", compute the Gini coefficient for each (this is similar to information gain), and split the node on the best column in the subset.\n",
    "\n",
    "This is essentially the same thing we did on the previous screen, but with far less typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731937714516\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement\n",
    "    # We set a random state to ensure we'll be able to replicate our results\n",
    "    # We set it to i instead of a fixed value so we don't get the same sample every time\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2,\n",
    "                                 splitter= 'random',max_features='auto')\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "combined = numpy.sum(predictions, axis=0) / 10\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735632444983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similar to decision trees, we can tweak some of the parameters for random forests, including:\n",
    "\n",
    "    min_samples_leaf\n",
    "    min_samples_split\n",
    "    max_depth\n",
    "    max_leaf_nodes\n",
    "\n",
    "These parameters apply to the individual trees in the model, and change how they are constructed. There are also parameters specific to the random forest that alter its overall construction:\n",
    "\n",
    "    n_estimators\n",
    "    bootstrap - \"Bootstrap aggregation\" is another name for bagging; this parameter indicates whether to turn it on (Defaults to True)\n",
    "\n",
    "Refer to the documentation for a full list of parameters.\n",
    "\n",
    "Tweaking parameters can increase the accuracy of the forest. The easiest tweak is to increase the number of estimators we use. This approach yields diminishing returns -- going from 10 trees to 100 will make a bigger difference than going from 100 to 500, which will make a bigger difference than going from 500 to 1000. The accuracy increase function is logarithmic, so increasing the number of trees beyond a certain number (usually 200) won't help much at all.\n",
    "\n",
    "Increase n_estimators to 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.748016353725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We were able to improve the AUC from 0.735 to 0.738, but the model using 150 trees took much longer to train. While the extra training time is trivial on the data set we're working with right now, understanding this trade-off will help you when you're working with much larger data sets where the extra time could amount to hours or days!\n",
    "\n",
    "One of the major advantages of random forests over single decision trees is that they tend to overfit less. Although each individual decision tree in a random forest varies widely, the average of their predictions is less sensitive to the input data than a single tree is. This is because while one tree can construct an incorrect and overfit model, the average of 100 or more trees will be more likely to hone in on the signal and ignore the noise. The signal will be the same across all of the trees, whereas each tree will hone in on the noise differently. This means that the average will discard the noise and keep the signal.\n",
    "\n",
    "In the following code cell, you'll see that we've fit a single decision tree to the training set, and made predictions for both the training and testing sets. The AUC for the training set predictions is .819, while the AUC for the testing set is .714. The fact that the test AUC is much lower than the train AUC indicates that the model is overfitting.\n",
    "\n",
    "Now let's train a similar random forest model and contrast it with what we just did.\n",
    "\n",
    "instructions\n",
    "\n",
    "    Fit clf to the training set and use it to make predictions on the training set.\n",
    "    Then, use it to make predictions on the testing set.\n",
    "    Print both AUC scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819001392492\n",
      "0.714688067838\n",
      "0.79249287474\n",
      "0.746842822532\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=5)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train[\"high_income\"], predictions))\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=5)\n",
    "\n",
    "clf.fit(train[columns],train['high_income'] )\n",
    "predictions_train = clf.predict( train[columns]) \n",
    "print( roc_auc_score( train['high_income'], predictions_train ) )\n",
    "\n",
    "predictions_test = clf.predict( test[columns])\n",
    "print( roc_auc_score( test['high_income'], predictions_test )  )    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the code cell from the previous screen, overfitting decreased with a random forest, and accuracy went up overall.\n",
    "\n",
    "While the random forest algorithm is incredibly powerful, it isn't applicable to all tasks. The main strengths of a random forest are:\n",
    "\n",
    "    Very accurate predictions - Random forests achieve near state-of-the-art performance on many machine learning tasks. Along with neural networks and gradient-boosted trees, they're typically one of the top-performing algorithms.\n",
    "    Resistance to overfitting - Due to their construction, random forests are fairly resistant to overfitting. We still need to set and tweak parameters like max_depth though.\n",
    "\n",
    "The main weaknesses of using a random forest are:\n",
    "\n",
    "    They're difficult to interpret - Because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "    They take longer to create - Making two trees takes twice as long as making one, making three takes three times as long, and so on. Fortunately, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier. We'll discuss parallelization in greater detail later on.\n",
    "\n",
    "Given these trade-offs, it makes sense to use random forests in situations where accuracy is of the utmost importance; being able to interpret or explain the decisions the model is making isn't key. In cases where time is of the essence or interpretability is important, a single decision tree may be a better choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
