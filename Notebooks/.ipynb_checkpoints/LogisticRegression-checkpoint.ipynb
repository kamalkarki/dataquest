{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAoiItLs/wPTQXdeZsX25QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efedc2b7390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Logistic Function\n",
    "def logistic(x):\n",
    "    # np.exp(x) raises x to the exponential power, ie e^x. e ~= 2.71828\n",
    "    return np.exp(x)  / (1 + np.exp(x)) \n",
    "    \n",
    "# Generate 50 real values, evenly spaced, between -6 and 6.\n",
    "x = np.linspace(-6,6,50, dtype=float)\n",
    "\n",
    "# Transform each number in t using the logistic function.\n",
    "y = logistic(x)\n",
    "\n",
    "# Plot the resulting data.\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous course, we explored a supervised machine learning technique called linear regression. Linear regression works well when the target column we're trying to predict, the dependent variable, is ordered and continuous. If the target column instead contains discrete values, then linear regression isn't a good fit.\n",
    "\n",
    "In this mission, we'll explore how to build a predictive model for these types of problems, which are known as classification problems. In classification, our target column has a finite set of possible values which represent different categories a row can belong to. We use integers to represent the different categories so we can continue to use mathematical functions to describe how the independent variables map to the dependent variable. Here are a few examples of classification problems:\n",
    "\n",
    "    Problem \t                        Sample Features \t      Type       \tCategories        \t Numerical Categories\n",
    "     Should we accept this student     College GPA, SAT Score,    Binary      Don't Accept, Accept     0,1\n",
    "                                     Quality of Recommendations\n",
    "     based on their graduate school \n",
    "     application? \t  \t \t \t\n",
    "    What is the most likely blood \n",
    "    type of 2 parent's offspring? \tParent 1's blood type,\n",
    "                                        Parent 2's blood type. \tMulti-class     A, B, AB,                O 1, 2,3,4\n",
    "\n",
    "We'll focus on binary classification for now, where the only 2 options for values are:\n",
    "\n",
    "    0 for the False condition,\n",
    "    1 for the True condition.\n",
    "\n",
    "Before we dive into classification, let's understand the data we'll be working with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "admissions = pd.read_csv('/home/kamlesh/AnacondaProjects/Dataquest/Data/admissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit       gpa         gre\n",
       "0      0  3.177277  594.102992\n",
       "1      0  3.412655  631.528607\n",
       "2      0  2.728097  553.714399\n",
       "3      0  3.093559  551.089985\n",
       "4      0  3.141923  537.184894"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can use both the gpa and gre columns to predict the admit column, we'll focus on using just the gpa column to keep things simple. Let's read the data into Pandas and visualize the relationship between gpa and admit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efedc29ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(admissions['gpa'],admissions['admit'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous scatter plot, you'll notice that the gpa column and the admit column do not have a clear linear relationship. Recall that the admit column only contains the values 0 and 1 and are used to represent binary values and the numbers themselves don't carry any weight. When numbers are used to represent different options or categories, they are referred to as categorical values. Classification focuses on estimating the relationship between the independent variables and the dependent, categorical variable.\n",
    "\n",
    "In this mission, we'll focus on a classification technique called logistic regression. While a linear regression model outputs a real number as the label, a logistic regression model outputs a probability value. In binary classification, if the probability value is larger than a certain threshold probability, we assign the label for that row to 1 or 0 otherwise.\n",
    "\n",
    "This threshold probability is something we select, and we'll learn about how to select a good threshold probability in later missions. For now, let's dive more into how logistic regression works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we used the linear function y = mx + b to represent the relationship between the independent variables and the dependent variable. In logistic regression, we use the logistic function, which is a version of the linear function that is adapted for classification.\n",
    "\n",
    "Let's explore some of the logistic function's properties to better understand why it's useful for classification tasks. Unlike in linear regression, where the output can be any real value, in logistic regression the output has to be a real value between 0 and 1, since the output represents a probability value. Note that the model can't output a negative value or it would violate this criteria.\n",
    "\n",
    "Here's the mathematical representation of the logistic function:\n",
    "\n",
    "σ(t)=et/1+et ( correct it )\n",
    "\n",
    "The logistic function is broken up into 2 key parts:\n",
    "\n",
    "    The exponential transformation, transforming all values to be positive:\n",
    "\n",
    "et\n",
    "\n",
    "    The normalization transformation, transforming all values to range between 0 and 1:\n",
    "\n",
    "t1+t\n",
    "\n",
    "The exponential transformation and the normalization forces the output values to be squeezed between 0 and 1. If you plot just the exponential part, the output values won't be limited to the range of 0 to 1:\n",
    "\n",
    "Imgur\n",
    "\n",
    "The normalization part alone unfortunately isn't sufficient since it struggles at constraining negative values:\n",
    "\n",
    "Imgur\n",
    "\n",
    "Together, both of these pieces work well to constrain the output to the range of values that probabilities are in. In the following code cell, we plot the logistic function to visualize its properties. Specifically, we:\n",
    "\n",
    "    define the logistic() function using the NumPy exp function,\n",
    "    generate equally spaced values, between -6 and 6 to represent the x-axis,\n",
    "    calculate the y-axis values by feeding each value in x to the logistic() function,\n",
    "    creating a line plot to visualize x and y.\n",
    "\n",
    "The generated plot will demonstrate that the y-axis values are constrained to range from 0 to 1. We encourage you to plot more x-values to confirm that this holds true for all potential x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAoiItLs/wPTQXdeZsX25QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efedc2bd908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Logistic Function\n",
    "def logistic(x):\n",
    "    # np.exp(x) raises x to the exponential power, ie e^x. e ~= 2.71828\n",
    "    return np.exp(x)  / (1 + np.exp(x)) \n",
    "    \n",
    "# Generate 50 real values, evenly spaced, between -6 and 6.\n",
    "x = np.linspace(-6,6,50, dtype=float)\n",
    "\n",
    "# Transform each number in t using the logistic function.\n",
    "y = logistic(x)\n",
    "\n",
    "# Plot the resulting data.\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move onto training the logistic regression model using our dataset. We won't dive into the math and the steps required to fit a logistic regression model to the training data in this mission. We'll instead focus on using the scikit-learn library to fit a model between the gpa and admit columns. Recall that the gpa column contains the GPA of each applicant as a real value between 0.0 and 4.0 and the admit column specifies if that applicant was admitted (0 if not admitted and 1 if admitted). Since we're only working with one feature, gpa, this is referred to as a univariate model.\n",
    "\n",
    "Training a logistic regression model in scikit-learn is similar to training a linear regression model, with the key difference that we use the LogisticRegression class instead of the LinearRegression class. Scikit-learn was designed to make it easy to swap out models by keeping the syntax and notation as consistent as possible across its different classes.\n",
    "instructions\n",
    "\n",
    "    Import the LogisticRegression class and instantiate a model named logistic_model.\n",
    "    Use the LogisticRegression method fit to fit the model to the data. We're only interested in constructing a model that uses gpa values to predict admit values.\n",
    "    View the documentation for the LogisticRegression class if you get stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that the output of a logistic regression model is the probability that the row should be labelled as True, or in our case 1. We can use the trained model to return the predicted probability for each row in the training data.\n",
    "\n",
    "To return the predicted probability, use the predict_proba method. The only required parameter for this method is the num_features by num_sample matrix of observations we want scikit-learn to return predicted probabilities for. For each input row, scikit-learn will return a NumPy array with 2 probability values:\n",
    "\n",
    "    the probability that the row should be labelled 0,\n",
    "    the probability that the row should be labelled 1.\n",
    "\n",
    "Since 0 and 1 are the only 2 possible categories and represent the entire outcome space, these 2 probabilities will always add upto 1.\n",
    "\n",
    "probabilities = logistic_model.predict_proba(admissions[[\"gpa\"]])\n",
    "\n",
    "#### Probability that the row belongs to label `0`.\n",
    "\n",
    "probabilities[:,0]\n",
    "\n",
    "#### Probabililty that the row belongs to label `1`.\n",
    "\n",
    "probabilities[:,1]\n",
    "\n",
    "Let's use this method to return the probability for each student that they would be admitted and then visualize the results on a scatter plot.\n",
    "instructions\n",
    "\n",
    "    Use the LogisticRegression method predict_proba to return the predicted probabilities for the data in the gpa column. Assign the returned probabilities to pred_probs.\n",
    "\n",
    "    Create and display a scatter plot using the Matplotlib scatter function where:\n",
    "        the x-axis is the values in the gpa column,\n",
    "        the y-axis is the probability of being classified as label 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "pred_probs = logistic_model.predict_proba( admissions[['gpa']] )\n",
    "print( type( pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7efedc9cfe10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJJWQ4S5JJWS4S1IJGe6SVEKGuySV0P8H73wqzV5Oan4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efee8f2bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print( pred_probs)\n",
    "\n",
    "plt.scatter( admissions['gpa'] , pred_probs[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7efedc98acc0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuVHYrLXnW2AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efedca9d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "\n",
    "fitted_labels = logistic_model.predict(admissions[['gpa']] )\n",
    "plt.scatter( admissions['gpa'],fitted_labels )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to evaluating binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructions\n",
    "\n",
    "    Use the LogisticRegression method predict to return the label for each observation in the dataset, admissions. Assign the returned list to labels.\n",
    "    Add a new column to the admissions Dataframe named predicted_label that contains the values from labels.\n",
    "    Use the Series method value_counts and the print function to display the distribution of the values in the predicted_label column.\n",
    "    Use the Dataframe method head and the print function to display the first 5 rows in admissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "\n",
    "labels = model.predict( admissions[['gpa']] )\n",
    "admissions[ 'predicted_label'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    598\n",
       "1     46\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The admissions Dataframe now contains the predicted value for that row, in the predicted_label column, and the actual value for that row, in the admit column. This format makes it easier for us to calculate how effective the model was on the training data. The simplest way to determine the effectiveness of a classification model is prediction accuracy. Accuracy helps us answer the question:\n",
    "\n",
    "    What fraction of the predictions were correct (actual label matched predicted label)?\n",
    "\n",
    "Prediction accuracy boils down to the number of labels that were correctly predicted divided by the total number of observations:\n",
    "\n",
    "        Accuracy=# of Correctly Predicted/# of Observations\n",
    "\n",
    "In logistic regression, recall that the model's output is a probability between 0 and 1. To decide who gets admitted, we set a threshold and accept all of the students where their computed probability exceeds that threshold. This threshold is called the discrimination threshold and scikit-learn sets it to 0.5 by default when predicting labels. If the predicted probability is greater than 0.5, the label for that observation is 1. If it is instead less than 0.5, the label for that observation is 0.\n",
    "\n",
    "An accuracy of 1.0 means that the model predicted 100% of admissions correctly for the given discrimination threshold. An accuracy of 0.2 means that the model predicted 20% of the admissions correctly. Let's calculate the accuracy for the predictions the logistic regression model made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructions\n",
    "\n",
    "    Rename the admit column from the admissions Dataframe to actual_label so it's more clear which column contains the predicted labels (predicted_label) and which column contains the actual labels (actual_label).\n",
    "    Compare the predicted_label column with the actual_label column.\n",
    "        Use a double equals sign (==) to compare the 2 Series objects and assign the resulting Series object to matches.\n",
    "    Use conditional filtering to filter admissions to just the rows where matches is True. Assign the resulting Dataframe to correct_predictions.\n",
    "        Display the first 5 rows in correct_predictions to make sure the values in the predicted_label and actual_label columns are equal.\n",
    "    Calculate the accuracy and assign the resulting float value to accuracy.\n",
    "        Display accuracy using the print function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual label</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GRE</th>\n",
       "      <th>Predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>1</td>\n",
       "      <td>3.381359</td>\n",
       "      <td>720.718438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1</td>\n",
       "      <td>3.083956</td>\n",
       "      <td>556.918021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>1</td>\n",
       "      <td>3.114419</td>\n",
       "      <td>734.297679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>1</td>\n",
       "      <td>3.549012</td>\n",
       "      <td>604.697503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1</td>\n",
       "      <td>3.532753</td>\n",
       "      <td>588.986175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual label       GPA         GRE  Predicted label\n",
       "639             1  3.381359  720.718438                0\n",
       "640             1  3.083956  556.918021                0\n",
       "641             1  3.114419  734.297679                0\n",
       "642             1  3.549012  604.697503                0\n",
       "643             1  3.532753  588.986175                0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions = admissions.rename(columns={\"admit\": \"Actual label\",'gpa':'GPA',\"gre\":\"GRE\",\"predicted_label\":\"Predicted label\"} )\n",
    "admissions.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1       True\n",
       "2       True\n",
       "3       True\n",
       "4       True\n",
       "5       True\n",
       "6       True\n",
       "7       True\n",
       "8       True\n",
       "9      False\n",
       "10      True\n",
       "11      True\n",
       "12      True\n",
       "13      True\n",
       "14      True\n",
       "15      True\n",
       "16      True\n",
       "17      True\n",
       "18      True\n",
       "19      True\n",
       "20      True\n",
       "21      True\n",
       "22      True\n",
       "23      True\n",
       "24      True\n",
       "25      True\n",
       "26      True\n",
       "27      True\n",
       "28      True\n",
       "29      True\n",
       "       ...  \n",
       "614     True\n",
       "615    False\n",
       "616    False\n",
       "617     True\n",
       "618    False\n",
       "619    False\n",
       "620    False\n",
       "621     True\n",
       "622    False\n",
       "623    False\n",
       "624    False\n",
       "625    False\n",
       "626    False\n",
       "627    False\n",
       "628     True\n",
       "629    False\n",
       "630    False\n",
       "631    False\n",
       "632    False\n",
       "633    False\n",
       "634     True\n",
       "635    False\n",
       "636     True\n",
       "637    False\n",
       "638    False\n",
       "639    False\n",
       "640    False\n",
       "641    False\n",
       "642    False\n",
       "643    False\n",
       "Length: 644, dtype: bool"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = admissions['Actual label'] == admissions[\"Predicted label\"]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual label</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GRE</th>\n",
       "      <th>Predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual label       GPA         GRE  Predicted label\n",
       "0             0  3.177277  594.102992                0\n",
       "1             0  3.412655  631.528607                0\n",
       "2             0  2.728097  553.714399                0\n",
       "3             0  3.093559  551.089985                0\n",
       "4             0  3.141923  537.184894                0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = admissions[matches]\n",
    "\n",
    "correct_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6459627329192547\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(correct_predictions) / len(admissions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It looks like the raw accuracy is around 64.6% which is better than randomly guessing the label (which would result in around a 50% accuracy). Calculating the accuracy of a model on the dataset used for training is a useful initial step just to make sure the model at least beats randomly assigning a label for each observation. However, prediction accuracy doesn't tell us much more.\n",
    "\n",
    "The accuracy doesn't tell us how the model performs on data it wasn't trained on. A model that returns a 100% accuracy when evaluated on it's training set doesn't tell us how well the model works on data it's never seen before (and wasn't trained on). Accuracy also doesn't help us discriminate between the different types of outcomes a binary classification model can make. In a later mission, we'll learn how to evaluate a model's effectiveness on new, unseen data. In this mission, we'll focus on the principles of evaluating binary classification models by testing our model's effectiveness on the training data.\n",
    "\n",
    "To start, let's discuss the 4 different outcomes of a binary classification model:\n",
    "\n",
    "    Prediction \tObservation\n",
    "                    Admitted (1) \tRejected (0)\n",
    "    Admitted (1) \tTrue Positive (TP) \tFalse Positive (FP)\n",
    "    Rejected (0) \tFalse Negative (FN) \tTrue Negative (TN)\n",
    "\n",
    "By segmenting a model's predictions into these different outcome categories, we can start to think about other measures of effectiveness that give us more granularity than simple accuracy.\n",
    "\n",
    "We can define these outcomes as:\n",
    "\n",
    "    True Positive - The model correctly predicted that the student would be admitted.\n",
    "        Said another way, the model predicted that the label would be Positive, and that ended up being True.\n",
    "        In our case, Positive refers to being admitted and maps to the label 1 in the dataset.\n",
    "        For this dataset, a true positive is whenever predicted_label is 1 and actual_label is 1.\n",
    "\n",
    "    True Negative - The model correctly predicted that the student would be rejected.\n",
    "        Said another way, the model predicted that the label would be Negative, and that ended up being True.\n",
    "        In our case, Negative refers to being rejected and maps to the label 0 in the dataset.\n",
    "        For this dataset, a true negative is whenever predicted_label is 0 and actual_label is 0.\n",
    "\n",
    "    False Positive - The model incorrectly predicted that the student would be admitted even though the student was actually rejected.\n",
    "        Said another way, the model predicted that the label would be Positive, but that was False (the actual label was False).\n",
    "        For this dataset, a false positive is whenever predicted_label is 1 but the actual_label is 0.\n",
    "\n",
    "    False Negative - The model incorrectly predicted that the student would be rejected even though the student was actually admitted.\n",
    "        Said another way, the model predicted that the would be Negative, but that was False (the actual value was True).\n",
    "        For this dataset, a false negative is whenever predicted_label is 0 but the actual_label is 1.\n",
    "\n",
    "Let's calculate the number of observations that fall into some of these outcome categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comapre = admissions['Actual label'] == admissions['Predicted label']\n",
    "print(type(comapre))\n",
    "len(comapre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for true in comapre:\n",
    "    if true == True:\n",
    "        count +=1\n",
    "true_positive = count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for true in comapre:\n",
    "    if true == False:\n",
    "        count +=1\n",
    "true_ngative = count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_ngative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "true_positive_filter = (admissions[\"Predicted label\"] == 1) & (admissions[\"Actual label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "\n",
    "true_negative_filter = (admissions[\"Predicted label\"] == 0) & (admissions[\"Actual label\"] == 0)\n",
    "true_negatives = len(admissions[true_negative_filter])\n",
    "\n",
    "print(true_positives)\n",
    "print(true_negatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a few measures that are much more insightful than simple accuracy. Let's start with sensitivity:\n",
    "\n",
    "    Sensitivity or True Positive Rate - The proportion of applicants that were correctly admitted:\n",
    "\n",
    "TPR=True Positives/( True Positives + False Negatives )\n",
    "\n",
    "Of all of the students that should have been admitted (True Positives + False Negatives), what fraction did the model correctly admit (True Positives)? More generally, this measure helps us answer the question:\n",
    "\n",
    "    How effective is this model at identifying positive outcomes?\n",
    "\n",
    "In our case, the positive outcome (label of 1) is admitting a student. If the True Positive Rate is low, it means that the model isn't effective at catching positive cases. For certain problems, high sensitivity is incredibly important. If we're building a model to predict which patients have cancer, every patient that is missed by the model could mean a loss of life. We want a highly sensitive model that is able to \"catch\" all of the positive cases (in this case, the positive case is a patient with cancer).\n",
    "\n",
    "Let's calculate the sensitivity for the model we're working with.\n",
    "instructions\n",
    "\n",
    "    Calculate the number of false negatives (where the model predicted rejected but the student was actually admitted) and assign to false_negatives.\n",
    "    Calculate the sensitivity and assign the computed value to sensitivity.\n",
    "    Display sensitivity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the previous screen\n",
    "true_positive_filter = (admissions[\"Predicted label\"] == 1) & ( admissions[\"Actual label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "\n",
    "#false_negative_filter rejected by the model but actually in\n",
    "false_negative_filter = (admissions[\"Predicted label\"] == 0 ) & (admissions[\"Actual label\"] == 1)\n",
    "    \n",
    "false_negatives = len ( admissions[false_negative_filter] ) \n",
    "#Now from the whole data set count the false negatives\n",
    "#false_negative_filter\n",
    "sensitivity = true_positives/ ( true_positives + false_negatives )\n",
    "#false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12704918032786885"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the sensitivity of the model is around 12.7% and only about 1 in 8 students that should have been admitted were actually admitted. In the context of predicting student admissions, this probably isn't too bad of a thing. Graduate schools can only admit a select number of students into their programs and by definition they end up rejecting many qualified students that would have succeeded.\n",
    "\n",
    "In the healthcare context, however, low sensitivity could mean a severe loss of life. If a classification model is only catching 12.7% of positive cases for an illness, then around 7 of 8 people are going undiagnosed (being classified as false negatives). Hopefully you're beginning to acquire a sense for the tradeoffs predictive models make and the importance of understanding the various measures.\n",
    "\n",
    "Let's now learn about specificity:\n",
    "\n",
    "    Specificity or True Negative Rate - The proportion of applicants that were correctly rejected:\n",
    "\n",
    "TNR=True Negatives/ ( False Positives+True Negatives)\n",
    "\n",
    "This helps us answer the question:\n",
    "\n",
    "    How effective is this model at identifying negative outcomes?\n",
    "\n",
    "In our case, the specificity tells us the proportion of applicants who should be rejected (actual_label equal to 0, which consists of False Positives + True Negatives) that were correctly rejected (just True Negatives). A high specificity means that the model is really good at predicting which applicants should be rejected.\n",
    "\n",
    "Let's calculate the specificity of our model.\n",
    "instructions\n",
    "\n",
    "    Calculate the number of false positives (where the model predicted admitted but the student was actually rejected) and assign to false_positives.\n",
    "    Calculate the specificity and assign the computed value to specificity.\n",
    "    Display specificity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    " #true negatives talks aboout rej\n",
    "#false positive = predicted_label 1 but actual_label = 0\n",
    "\n",
    "false_positive_filter = ( admissions[\"Predicted label\"]== 1 ) & ( admissions[\"Actual label\"] == 0 )\n",
    "\n",
    "false_positives = len( admissions[false_positive_filter])\n",
    "\n",
    "specificity = (true_negatives)/( true_negatives + false_positives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the specificity of the model is 96.25%. This means that the model is really good at knowing which applicants to reject. Since around only 7% of applicants were accepted that applied, it's important that the model reject people correctly who wouldn't have otherwise been accepted.\n",
    "\n",
    "In this mission, we learned about some of the different ways of evaluating how well a binary classification model performs. The different measures we learned about have very similar names and it's easy to confuse them. Don't fret! The important takeaway is the ability to frame the question you want to answer and working backwards from that to formulate the correct calculation.\n",
    "\n",
    "If you want to know how well a binary classification model is at catching positive cases, you should have the intuition to divide the correctly predicted positive cases by all actually positive cases. There are 2 outcomes associated with an admitted student (positive case), a false negative and a true positive. Therefore, by dividing the number of true positives by the sum of false negatives and true positives, you'll have the proportion corresponding to the model's effectiveness of identifying positive cases. While this proportion is referred to as the sensitivity, the word itself is secondary to the concept and being able to work backwards to the formula!\n",
    "\n",
    "These measures are just a starting point, however, and aren't super useful by themselves. In the next mission, we'll dive into cross-validation, where we'll evaluate our model's accuracy on new data that it wasn't trained on. In addition, we'll explore how varying the discrimination threshold affects the measures we learned about in this mission. These important techniques help us gain a much more complete understanding of a classification model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('/home/kamlesh/AnacondaProjects/Dataquest/Data/auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0       130.0  3504.0          12.0    70   \n",
       "1  15.0          8         350.0       165.0  3693.0          11.5    70   \n",
       "2  18.0          8         318.0       150.0  3436.0          11.0    70   \n",
       "3  16.0          8         304.0       150.0  3433.0          12.0    70   \n",
       "4  17.0          8         302.0       140.0  3449.0          10.5    70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 4 6 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(cars['cylinders'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130. 165. 150. 140. 198. 220. 215. 225. 190. 170. 160.  95.  97.  85.\n",
      "  88.  46.  87.  90. 113. 200. 210. 193. 100. 105. 175. 153. 180. 110.\n",
      "  72.  86.  70.  76.  65.  69.  60.  80.  54. 208. 155. 112.  92. 145.\n",
      " 137. 158. 167.  94. 107. 230.  49.  75.  91. 122.  67.  83.  78.  52.\n",
      "  61.  93. 148. 129.  96.  71.  98. 115.  53.  81.  79. 120. 152. 102.\n",
      " 108.  68.  58. 149.  89.  63.  48.  66. 139. 103. 125. 133. 138. 135.\n",
      " 142.  77.  62. 132.  84.  64.  74. 116.  82.]\n"
     ]
    }
   ],
   "source": [
    "print(cars['horsepower'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70 71 72 73 74 75 76 77 78 79 80 81 82]\n"
     ]
    }
   ],
   "source": [
    "print( cars['year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.  11.5 11.  10.5 10.   9.   8.5  8.   9.5 15.  15.5 16.  14.5 20.5\n",
      " 17.5 12.5 14.  13.5 18.5 13.  19.  19.5 18.  17.  23.5 16.5 21.  16.9\n",
      " 14.9 17.7 15.3 13.9 12.8 15.4 17.6 22.2 22.1 14.2 17.4 16.2 17.8 12.2\n",
      " 16.4 13.6 15.7 13.2 21.9 16.7 12.1 14.8 18.6 16.8 13.7 11.1 11.4 18.2\n",
      " 15.8 15.9 14.1 21.5 14.4 19.4 19.2 17.2 18.7 15.1 13.4 11.2 14.7 16.6\n",
      " 17.3 15.2 14.3 20.1 24.8 11.3 12.9 18.8 18.1 17.9 21.7 23.7 19.9 21.8\n",
      " 13.8 12.6 16.1 20.7 18.3 20.4 19.6 17.1 15.6 24.6 11.6]\n"
     ]
    }
   ],
   "source": [
    "print(cars['acceleration'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cylinders = pd.get_dummies(cars[\"cylinders\"], prefix=\"cyl\")\n",
    "cars = pd.concat([cars, dummy_cylinders], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_years = pd.get_dummies(cars[\"year\"], prefix=\"year\")\n",
    "cars = pd.concat([cars, dummy_years], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.drop(columns=[\"year\",\"cylinders\" ],axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>origin</th>\n",
       "      <th>cyl_3</th>\n",
       "      <th>cyl_4</th>\n",
       "      <th>cyl_5</th>\n",
       "      <th>cyl_6</th>\n",
       "      <th>...</th>\n",
       "      <th>year_73</th>\n",
       "      <th>year_74</th>\n",
       "      <th>year_75</th>\n",
       "      <th>year_76</th>\n",
       "      <th>year_77</th>\n",
       "      <th>year_78</th>\n",
       "      <th>year_79</th>\n",
       "      <th>year_80</th>\n",
       "      <th>year_81</th>\n",
       "      <th>year_82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displacement  horsepower  weight  acceleration  origin  cyl_3  cyl_4  \\\n",
       "0  18.0         307.0       130.0  3504.0          12.0       1      0      0   \n",
       "1  15.0         350.0       165.0  3693.0          11.5       1      0      0   \n",
       "2  18.0         318.0       150.0  3436.0          11.0       1      0      0   \n",
       "3  16.0         304.0       150.0  3433.0          12.0       1      0      0   \n",
       "4  17.0         302.0       140.0  3449.0          10.5       1      0      0   \n",
       "\n",
       "   cyl_5  cyl_6   ...     year_73  year_74  year_75  year_76  year_77  \\\n",
       "0      0      0   ...           0        0        0        0        0   \n",
       "1      0      0   ...           0        0        0        0        0   \n",
       "2      0      0   ...           0        0        0        0        0   \n",
       "3      0      0   ...           0        0        0        0        0   \n",
       "4      0      0   ...           0        0        0        0        0   \n",
       "\n",
       "   year_78  year_79  year_80  year_81  year_82  \n",
       "0        0        0        0        0        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        0        0        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    }
   ],
   "source": [
    "shuffled_rows = np.random.permutation(cars.index)\n",
    "shuffled_cars = cars.iloc[shuffled_rows]\n",
    "\n",
    "rows = int( cars.shape[0] * .70 )\n",
    "print(rows)\n",
    "train = shuffled_cars[ 0: rows ]\n",
    "\n",
    "test = shuffled_cars[rows: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unique_origins = cars[\"origin\"].unique()\n",
    "unique_origins.sort()\n",
    "\n",
    "features = [c for c in train.columns if c.startswith(\"cyl\") or c.startswith(\"year\")]\n",
    "\n",
    "models = {}\n",
    "for origin in unique_origins:\n",
    "    lg = LogisticRegression()\n",
    "    y = train[\"origin\"] == origin\n",
    "    lg.fit( train[features],y )\n",
    "    models[origin] = lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [1, 2, 3]\n",
       "Index: []"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_probs = pd.DataFrame(columns=unique_origins)\n",
    "testing_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "118\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "for origin in unique_origins:\n",
    "    probs = models[origin].predict_proba(test[features])\n",
    "    print( len(test))\n",
    "    for i in range( len(test)):\n",
    "        #testing_probs[origin][i] = probs[i,1]\n",
    "        #testing_probs.loc[i,origin] = probs[i,1] \n",
    "        #testing_probs[i][origin] = probs[i,1]\n",
    "        testing_probs.loc[i,origin] = probs[i,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.967696</td>\n",
       "      <td>0.0153986</td>\n",
       "      <td>0.0435586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.526436</td>\n",
       "      <td>0.139321</td>\n",
       "      <td>0.328511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443832</td>\n",
       "      <td>0.303841</td>\n",
       "      <td>0.234738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.526436</td>\n",
       "      <td>0.139321</td>\n",
       "      <td>0.328511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.644237</td>\n",
       "      <td>0.112064</td>\n",
       "      <td>0.261336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1          2          3\n",
       "0  0.967696  0.0153986  0.0435586\n",
       "1  0.526436   0.139321   0.328511\n",
       "2  0.443832   0.303841   0.234738\n",
       "3  0.526436   0.139321   0.328511\n",
       "4  0.644237   0.112064   0.261336"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_probs.shape\n",
    "testing_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduction operation 'argmax' not allowed for this dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-a8a830b69931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtesting_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Py35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36midxmax\u001b[0;34m(self, axis, skipna)\u001b[0m\n\u001b[1;32m   5837\u001b[0m         \"\"\"\n\u001b[1;32m   5838\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5839\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanargmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5840\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5841\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Py35/lib/python3.5/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'reduction operation {name!r} not allowed for this dtype'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reduction operation 'argmax' not allowed for this dtype"
     ]
    }
   ],
   "source": [
    "type(testing_probs)\n",
    "testing_probs.idxmax( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\", \"origin\", \"car name\"]\n",
    "cars = pd.read_csv('/home/kamlesh/AnacondaProjects/Dataquest/Data/auto-mpg.data',delim_whitespace= True, names= columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['130.0', '165.0', '150.0', '140.0', '198.0', '220.0', '215.0',\n",
       "       '225.0', '190.0', '170.0', '160.0', '95.00', '97.00', '85.00',\n",
       "       '88.00', '46.00', '87.00', '90.00', '113.0', '200.0', '210.0',\n",
       "       '193.0', '?', '100.0', '105.0', '175.0', '153.0', '180.0', '110.0',\n",
       "       '72.00', '86.00', '70.00', '76.00', '65.00', '69.00', '60.00',\n",
       "       '80.00', '54.00', '208.0', '155.0', '112.0', '92.00', '145.0',\n",
       "       '137.0', '158.0', '167.0', '94.00', '107.0', '230.0', '49.00',\n",
       "       '75.00', '91.00', '122.0', '67.00', '83.00', '78.00', '52.00',\n",
       "       '61.00', '93.00', '148.0', '129.0', '96.00', '71.00', '98.00',\n",
       "       '115.0', '53.00', '81.00', '79.00', '120.0', '152.0', '102.0',\n",
       "       '108.0', '68.00', '58.00', '149.0', '89.00', '63.00', '48.00',\n",
       "       '66.00', '139.0', '103.0', '125.0', '133.0', '138.0', '135.0',\n",
       "       '142.0', '77.00', '62.00', '132.0', '84.00', '64.00', '74.00',\n",
       "       '116.0', '82.00'], dtype=object)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning the data set\n",
    "cars['horsepower'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cars = cars[cars['horsepower'] !='?'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
       "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
       "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
       "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
       "3  16.0          8         304.0      150.0  3433.0          12.0          70   \n",
       "4  17.0          8         302.0      140.0  3449.0          10.5          70   \n",
       "\n",
       "   origin                   car name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cars['horsepower'] = filtered_cars['horsepower'].astype( dtype =float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn\n",
    "\n",
    "At the heart of understanding overfitting is understanding bias and variance. Bias and variance make up the 2 observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance.\n",
    "\n",
    "In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discussed before how overfitting generally happens when a model performs well on a training set but doesn't generalize well to new data. A key nuance here is that you should think of overfitting as a relative term. Between any 2 models, one will overfit more than the other one.\n",
    "\n",
    "Understanding the bias variance tradeoff is critical to understanding overfitting. Every process has some amount of inherent noise that's unobservable. Overfit models tend to capture the noise as well as the signal in a dataset.\n",
    "\n",
    "Scott Fortman Roe's blog post on the bias-variance tradeoff has a wonderful image that describes this tradeoff:\n",
    "\n",
    "Imgur\n",
    "\n",
    "We can approximate the bias of a model by training a few different models from the same class (linear regression in this case) using different features on the same dataset and calculating their error scores. For regression, we can use mean absolute error, mean squared error, or R-squared.\n",
    "\n",
    "We can calculate the variance of the predicted values for each model we train and we'll observe an increase in variance as we build more complex, multivariate models.\n",
    "\n",
    "While an extremely simple, univariate linear regression model will underfit, an extremely complicated, multivariate linear regression model will overfit. Depending on the problem you're working on, there's a happy middle ground that will help you construct reliable and useful predictive models.\n",
    "\n",
    "Let's first create a function that we can use for training the model and computing the bias and variance values and use it to train some simple, univariate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructions\n",
    "\n",
    "    Create a function named train_and_test that:\n",
    "        Takes in a list of column names in filtered_cars as the sole parameter (cols),\n",
    "        Trains a linear regression model using:\n",
    "            The columns in cols as the features,\n",
    "            The mpg column as the target variable.\n",
    "        Uses the trained model to make predictions using the same input it was trained on,\n",
    "        Computes the variance of the predicted values and the mean squared error between the predicted values and the actual label (mpg column).\n",
    "        Returns the mean squared error value followed by the variance (e.g. return(mse, variance)).\n",
    "\n",
    "    Use the train_and_test function to train a model using only the cylinders column. Assign the resulting mean squared error value and variance to cyl_mse and cyl_var.\n",
    "\n",
    "    Use the train_and_test function to train a model using only the weight column. Assign the resulting mean squared error value and variance to weight_mse and weight_var.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.tolist of Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
       "       'acceleration', 'model year', 'origin', 'car name'],\n",
       "      dtype='object')>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_cars.columns.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = {}\n",
    "features = filtered_cars.columns.tolist\n",
    "def train_and_test(cols):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit( filtered_cars[cols],filtered_cars['mpg'] )\n",
    "    predictions = lr.predict( filtered_cars[cols] )\n",
    "    mse = mean_squared_error( predictions, filtered_cars['mpg'] )       \n",
    "    variance = np.var( predictions)\n",
    "    return( mse, variance )\n",
    "\n",
    "#mse1,vars1 = train_and_test( features )\n",
    "cyl_mse,cyl_var = train_and_test( ['cylinders'])\n",
    "weight_mse,weight_var = train_and_test(['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "\n",
    "Use the train_and_test function to train linear regression models using the following columns as the features:\n",
    "\n",
    "    columns: cylinders, displacement.\n",
    "        MSE: two_mse, variance: two_var.\n",
    "    columns: cylinders, displacement, horsepower.\n",
    "        MSE: three_mse, variance: three_var.\n",
    "    columns: cylinders, displacement, horsepower, weight.\n",
    "        MSE: four_mse, variance: four_var.\n",
    "    columns: cylinders, displacement, horsepower, weight, acceleration.\n",
    "        MSE: five_mse, variance: five_var.\n",
    "    columns: cylinders, displacement, horsepower, weight, acceleration, model year\n",
    "        MSE: six_mse, variance: six_var.\n",
    "    columns: cylinders, displacement, horsepower, weight, acceleration, model year, origin\n",
    "        MSE: seven_mse, variance: seven_var.\n",
    "\n",
    "Use print statements or the variable inspector to display each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_mse, one_var = train_and_test([\"cylinders\"])\n",
    "two_mse,two_var = train_and_test(['cylinders','displacement'])\n",
    "three_mse , three_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def train_and_cross_val(cols):\n",
    "    features = filtered_cars[cols]\n",
    "    target = filtered_cars[\"mpg\"]\n",
    "    \n",
    "    variance_values = []\n",
    "    mse_values = []\n",
    "    \n",
    "    # KFold instance.\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=3)\n",
    "    \n",
    "    # Iterate through over each fold.\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        # Training and test sets.\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "        \n",
    "        # Fit the model and make predictions.\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        predictions = lr.predict(X_test)\n",
    "        \n",
    "        # Calculate mse and variance values for this fold.\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        var = np.var(predictions)\n",
    "\n",
    "        # Append to arrays to do calculate overall average mse and variance values.\n",
    "        variance_values.append(var)\n",
    "        mse_values.append(mse)\n",
    "   \n",
    "    # Compute average mse and variance values.\n",
    "    avg_mse = np.mean(mse_values)\n",
    "    avg_var = np.mean(variance_values)\n",
    "    return(avg_mse, avg_var)\n",
    "        \n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "#three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "#four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "#five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "#six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "#seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efedc976128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter([2,3,4,5,6,7], [two_mse, three_mse, four_mse, five_mse, six_mse, seven_mse], c='red')\n",
    "plt.scatter([2,3,4,5,6,7], [two_var, three_var, four_var, five_var, six_var, seven_var], c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
